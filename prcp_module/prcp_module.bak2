#include <linux/migrate.h>
#include <linux/memory_hotplug.h>
#include <linux/pagemap.h>
#include <linux/rmap.h>
#include <linux/init.h>
#include <linux/cpu.h>
#include <linux/percpu.h>
#include <linux/module.h>
#include <linux/kthread.h>
#include <linux/wait.h>
#include <linux/kernel.h>
#include <linux/ioport.h>
#include <asm/io.h>
#include <linux/delay.h>
#include <linux/sched.h>
#include <linux/moduleparam.h>
#include <linux/mm.h>
#include <asm/pgtable.h>
#include <linux/spinlock.h>
#include <linux/slab.h>
#include <linux/mm.h>
#include <asm-generic/pgtable.h>

//[DCSLAB]
#include <linux/lockdep.h>
#include <asm/tlbflush.h>
#include <linux/time.h>
#include <linux/mmu_notifier.h>
#include <linux/preempt.h>
#include "../kernel/sched/sched.h"
#include "../mm/internal.h"
#include <linux/mm_inline.h>
#include <linux/migrate.h>
#include <linux/migrate_mode.h>
#include <linux/hugetlb.h>
#include <linux/random.h>


//#define DEBUG_FILE 1
#ifdef DEBUG_FILE
#include <linux/file.h>
#include <linux/fs.h>
#include <linux/syscalls.h>
#include <linux/fcntl.h>
#include <asm/uaccess.h>
#endif

//#define DEBUG_TIME
#define DEBUG 1
#define WORKING_SET_TOO_LARGE 1
#define LOCALITY_JUMPING 0

#define NR_CORES_PER_NODE 2
#define NR_NODES_IN_SYSTEM 1
#define NR_COLORS_IN_CACHE NUM_OF_COLORS
//#define DEBUG_DURATION 128
#define SCAN_TIME_WINDOW_US 2000
#define SCAN_FREQUENCY_MS 100
//#define SCAN_FREQUENCY_MS 200
#define MIGRATION_DELAY_UNIT 1000
//#define MIGRATION_DELAY_UNIT 500
//#define MIGRATION_DELAY_UNIT 200
#define WORKER_TIMEOUT 5
#define MANAGER_TIMEOUT 30

#define MIGRATION_INTERVAL 16
#define MIGRATION_THRESHOLD 40
#define JUST_BEFORE_DECISION (MIGRATION_INTERVAL - 1)

#define NR_PAGES_PER_COLOR_IN_MEMORY 32768
#define NR_WAYS_IN_CACHE 24
//#define NR_MIGRATE_PAGES_AT_ONCE (256)
#define NR_MIGRATE_PAGES_AT_ONCE (65536)
#define NO_COLOR (-1)
#define PRCP_NO_LIMIT (-1)

enum WORKER_STATE
{
	WORKER_WAIT,
	WORKER_START,
	WORKER_EXIT,
	WORKER_TERMINATED
};

enum PRCP_FLAG
{
	FLAG_DEFAULT,
	FLAG_NOT_MOVABLE,
	FLAG_NOT_FIT_IN_CACHE_CHECK,
	FLAG_NOT_FIT_IN_CACHE,
	FLAG_NORMAL,
	FLAG_POLLUTE
};

enum PRCP_STAGE
{
	STAGE_DEFAULT,
	STAGE_PREPARE_DECISION,
	STAGE_PAGE_MIGRATION
};

extern struct task_struct *get_rq_task(int cpu);
extern struct page *prcp_alloc_migrate_target(struct page *page, unsigned long demanded_color, int **resultp);
extern int prcp_do_migrate_range(struct task_struct *tsk, struct vm_area_struct *vma, unsigned long start_pfn, unsigned long end_pfn);
extern int prcp_try_isolate_lru_page(struct vm_area_struct *vma, unsigned long pfn);
struct task_struct *prcp_manager;

static char *output;
unsigned int prcp_stage = STAGE_DEFAULT;
unsigned int DELAY_WEIGHT = 1;

int HIGH_THRESHOLD = MIGRATION_INTERVAL / 2;
int LOW_THRESHOLD = 2;
int TOO_LARGE_THRESHOLD = MIGRATION_INTERVAL / 2;
int NR_POLLUTE_COLORS;
int NR_NORMAL_COLORS; // == POLLUTE_COLOR
int local_highest_access_cnt[NR_CORES_PER_NODE];
int local_sum_access_cnt[NR_CORES_PER_NODE];
int local_nr_pages[NR_CORES_PER_NODE];
int local_nr_normal_pages[NR_CORES_PER_NODE];
int local_nr_pollute_pages[NR_CORES_PER_NODE];
int local_nr_migrated_pages[NR_CORES_PER_NODE];

static inline int get_access_cnt(struct page *page)
{
	return atomic_read(&page->access_cnt);
}

static inline int get_difference_between_access_cnts(int access_cnt1, int access_cnt2)
{
	int difference = (access_cnt1 > access_cnt2)? (access_cnt1 - access_cnt2) : (access_cnt2 - access_cnt1);
	return difference;
}

static inline int get_prcp_flags(struct page *page)
{
	return page->prcp_flags;
}

static inline void init_access_cnt(struct page *page)
{
	atomic_set(&page->access_cnt, 0);
}

static inline void increase_access_cnt(struct page *page)
{
	atomic_inc(&page->access_cnt);
}

static inline void init_prcp_flags(struct page *page)
{
	page->prcp_flags = 0;
}

static inline void set_prcp_flags(struct page *page, int flags)
{
	page->prcp_flags = flags;
}

static inline int get_total_nr_migrated_pages(void)
{
	int i;
	int total = 0;
	for (i=0; i<NR_CORES_PER_NODE; i++) {
		total += local_nr_migrated_pages[i];
	}
	return total;
}

static inline int pick_pollute_color(void)
{
	/*
	int color;
	for (color=NR_NORMAL_COLORS; color < NR_COLORS_IN_CACHE; color++) {
		if (NR_NORMAL_COLORS
	}
	*/
	return 0;
}

static inline void update_nr_colors(int new_nr_pollute_colors)
{
	NR_POLLUTE_COLORS = new_nr_pollute_colors;
	NR_NORMAL_COLORS = NR_COLORS_IN_CACHE - new_nr_pollute_colors;
}

static inline void grow_pollute_buffer(void)
{
	if (NR_POLLUTE_COLORS < NR_COLORS_IN_CACHE - 1)
		update_nr_colors(NR_POLLUTE_COLORS + 1);
}

static inline void shrink_pollute_buffer(void)
{
	if (NR_POLLUTE_COLORS > 1)
		update_nr_colors(NR_POLLUTE_COLORS - 1);
}

static inline void adjust_pollute_buffer(void)
{
	int core;
	int total_nr_pollute_pages = 0;
	int total_nr_normal_pages = 0;
	for (core = 0; core < NR_CORES_PER_NODE; core++) {
		total_nr_pollute_pages += local_nr_pollute_pages[core];
		total_nr_normal_pages += local_nr_normal_pages[core];
	}
	if (total_nr_pollute_pages > ((NR_POLLUTE_COLORS * NR_PAGES_PER_COLOR_IN_MEMORY) / 2)) {
#if DEBUG > 0
		printk("[adjust_pollute_buffer] grow_pollute_buffer: %d -> ", NR_POLLUTE_COLORS);
#endif	
		grow_pollute_buffer();
#if DEBUG > 0
		printk("%d\n", NR_POLLUTE_COLORS);
#endif	
	} else if (total_nr_normal_pages > ((NR_NORMAL_COLORS * NR_PAGES_PER_COLOR_IN_MEMORY) / 2)) {
#if DEBUG > 0
		printk("[adjust_pollute_buffer] shrink_pollute_buffer: %d -> ", NR_POLLUTE_COLORS);
#endif	
		shrink_pollute_buffer();
#if DEBUG > 0
		printk("%d\n", NR_POLLUTE_COLORS);
#endif	
	}
}

static inline void update_threshold(void)
{
	int core;
	int total_highest = 0;
	int total_sum = 0;
	int total_nr_pages = 0;
	int total_average = 0;
	int temp;
//	int weight;
	for (core = 0; core < NR_CORES_PER_NODE; core++) {
		if (total_highest < local_highest_access_cnt[core]) {
			total_highest = local_highest_access_cnt[core];
		}
		total_sum += local_sum_access_cnt[core];
		total_nr_pages += local_nr_pages[core];
	}
	if (total_nr_pages != 0) {
		total_average = total_sum / total_nr_pages;
		temp = (total_highest + total_highest + total_average) / 3;
		//HIGH_THRESHOLD = (temp < 10)? 10 : temp;
		HIGH_THRESHOLD = temp;
		LOW_THRESHOLD = (total_average > 4)? (total_average / 2) : 2;
		TOO_LARGE_THRESHOLD = total_average;
		//LOW_THRESHOLD = (total_average > 1)? total_average : 2;
#if 0
		weight = DELAY_WEIGHT / 2;	
		HIGH_THRESHOLD += weight;
		LOW_THRESHOLD += weight;
#endif
		if (HIGH_THRESHOLD <= LOW_THRESHOLD)
			HIGH_THRESHOLD = LOW_THRESHOLD;
	}
#if DEBUG > 0
	printk("[update_threshold] HIGH_THRESHOLD: %d, LOW_THRESHOLD: %d, TOO_LARGE_THRESHOLD: %d\n", HIGH_THRESHOLD, LOW_THRESHOLD, TOO_LARGE_THRESHOLD);
#endif
}

#ifdef DEBUG_FILE
struct file* fp;
loff_t offset = 0;
char buf[1024];
#endif

module_param(output, charp, 0);

#ifdef DEBUG_FILE
static struct file* file_open(const char *path, int flags, int rights)
{
	struct file* filp = NULL;
	mm_segment_t oldfs;
	int err = 0;

	oldfs = get_fs();
	set_fs(get_ds());
	filp = filp_open(path, flags, rights);
	set_fs(oldfs);
	if (IS_ERR(filp)) {
		err = PTR_ERR(filp);
		return NULL;
	}
	return filp;
}

static void file_close(struct file* file)
{
	filp_close(file, NULL);
}

static int file_write(struct file* file, unsigned char* data, unsigned int size, loff_t* off)
{
	mm_segment_t oldfs;
	int ret;
	oldfs = get_fs();
	set_fs(get_ds());

	ret = vfs_write(file, data, size, off);

	set_fs(oldfs);
	return ret;
}
#endif

struct prcp_global_data {
	atomic_t thread_count;
	wait_queue_head_t manager_wq;
};

struct prcp_private_data {
	unsigned int worker_state;
	wait_queue_head_t worker_wq;
};

struct prcp_worker_args {
	struct prcp_global_data *global;
	struct prcp_private_data *private;
	struct task_struct *target_tsk;
};

static inline void prcp_init_private_date(struct prcp_private_data *private)
{
	private->worker_state = WORKER_WAIT;
	init_waitqueue_head(&private->worker_wq);
}

static inline void prcp_init_global_data(struct prcp_global_data *global)
{
	atomic_set(&global->thread_count, NR_CORES_PER_NODE);
	init_waitqueue_head(&global->manager_wq);
}

static inline void prcp_init_worker_args(struct prcp_worker_args *data, struct prcp_global_data *global, struct prcp_private_data *private, int nr_cores_per_node)
{
	int i;
	prcp_init_global_data(global);
	for (i=0; i<nr_cores_per_node; i++) {
		prcp_init_private_date(&private[i]);
		data[i].global = global;
		data[i].private = &private[i];
		data[i].target_tsk = NULL;
	}
}

static inline int prcp_check_all_worker_terminated(struct prcp_private_data *private)
{
	int i;
	for (i=0; i<NR_CORES_PER_NODE; i++) {
		if (private[i].worker_state != WORKER_TERMINATED) {
			return 0;
		}
	}
	return 1;
}

static inline signed long long get_time(void)
{
	struct timespec ts;
	getnstimeofday(&ts);
	return timespec_to_ns(&ts);
}

static inline unsigned int prcp_get_color(unsigned long pfn)
{
	return pfn & (NR_COLORS_IN_CACHE - 1);
}

static inline pte_t* get_pte_from_vpa(struct mm_struct *target_mm, unsigned long target_address)
{
	struct mm_struct *mm = target_mm;
	unsigned long address = target_address;
    pgd_t *pgd;
    pud_t *pud;
    pmd_t *pmd;

	pgd = pgd_offset(mm, address);
	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
		return NULL;
	pud = pud_offset(pgd, address);
	if (pud_none(*pud) || unlikely(pud_bad(*pud)))
		return NULL;
	pmd = pmd_offset(pud, address);
	if (pmd_none(*pmd) || unlikely(pmd_bad(*pmd)))
		return NULL;
	return pte_offset_map(pmd, address);
}

static inline pte_t* get_pte_from_vpa_lock(struct mm_struct *target_mm, unsigned long target_address, spinlock_t **ptl)
{
	struct mm_struct *mm = target_mm;
	unsigned long address = target_address;
    pgd_t *pgd;
    pud_t *pud;
    pmd_t *pmd;

	pgd = pgd_offset(mm, address);
	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
		return NULL;
	pud = pud_offset(pgd, address);
	if (pud_none(*pud) || unlikely(pud_bad(*pud)))
		return NULL;
	pmd = pmd_offset(pud, address);
	if (pmd_none(*pmd) || unlikely(pmd_bad(*pmd)))
		return NULL;
	return pte_offset_map_lock(mm, pmd, address, ptl);
}

static inline int is_pollute_color(unsigned int color)
{
	unsigned int i;
	for (i = NR_NORMAL_COLORS; i < NR_COLORS_IN_CACHE; i++) {
		if (color == i) {
			return 1;
		}
	}
	return 0;
}

static inline void INIT_LIST_HEADS(struct list_head *list, unsigned int nr_lists)
{
	int i;
	for (i = 0; i < nr_lists; i++) {
		INIT_LIST_HEAD(&list[i]);
	}
}

static inline void init_working_set_too_large_detector(unsigned long *start_addr, unsigned long *end_addr, int *working_set_size, int *start_access_cnt)
{
	*start_addr = 0;
	*end_addr = 0;
	*working_set_size = 0;
	*start_access_cnt = 0;
}

static inline int check_vma_skippable(struct vm_area_struct *vma)
{
#if 0
		if ((vma->vm_flags & (VM_WRITE|VM_EXEC)) == (VM_EXEC)) {// skip text_area
#if DEBUG > 1
			printk("prcp_page_migration: %s skip text area\n", tsk->comm);
#endif
			return 1;
		}
		if (vma->vm_flags & VM_SHARED) {
#if DEBUG > 1
			printk("prcp_page_migration: %s skip shared area\n", tsk->comm);
#endif
			return 1;
		}
		if (vma->vm_file && (vma->vm_flags & (VM_READ|VM_WRITE)) == (VM_READ)) {// skip read-only file-backed mappings
#if DEBUG > 1
			printk("prcp_page_migration: %s skip read-only file-backed mappings\n", tsk->comm);
#endif
			return 1;
		}
#if DEBUG > 1
		printk("prcp_page_migration: %s vma selected for migration\n", tsk->comm);
#endif
#endif
		return 0;
}

static inline int prcp_detect_working_set_too_large(struct mm_struct *mm, struct task_struct *tsk, struct vm_area_struct *vma, int working_set_size, unsigned long start_addr, unsigned long end_addr, struct list_head *migration_list)
{
	unsigned long pfn;
	unsigned long address;
	int color_idx;
	int nr_migrated_pollute_pages = 0;
	struct page *page;
	pte_t *ptep;
	spinlock_t *ptl;
	if (working_set_size > NR_MIGRATE_PAGES_AT_ONCE) 
		return 0;
	if (working_set_size > NR_WAYS_IN_CACHE * NR_NORMAL_COLORS) {
#if DEBUG > 0
		printk("[%s]prcp_detect_working_set_too_large: try to isolate %d Not-Fit-In-Cache pages from %lu to %lu\n", tsk->comm, working_set_size, start_addr, end_addr);
#endif
#if DEBUG > 0
		printk("core[0]'s highest access cnt: %d, core[1]'s highest access cnt: %d\n", local_highest_access_cnt[0], local_highest_access_cnt[1]);
#endif
		for (address = start_addr; address < end_addr; address+=PAGE_SIZE) {
			//ptep = get_pte_from_vpa(mm, address);
			ptep = get_pte_from_vpa_lock(mm, address, &ptl);
			if (ptep == NULL) 
				break;
			pfn = pte_pfn(*ptep);
			page = pfn_to_page(pfn);
			if (get_prcp_flags(page) == FLAG_NOT_FIT_IN_CACHE_CHECK) {
				if (prcp_try_isolate_lru_page(vma, pfn)) {
					color_idx = NR_NORMAL_COLORS + (nr_migrated_pollute_pages % NR_POLLUTE_COLORS);
					nr_migrated_pollute_pages++;
					set_prcp_flags(page, FLAG_NOT_FIT_IN_CACHE);
					page->demanded_color = color_idx;
					list_add_tail(&page->lru, migration_list);
#if DEBUG > 0
					if (address == start_addr) {
						printk("[%s]prcp_detect_working_set_too_large: ", tsk->comm);
					}
					printk("(%lu, %lu, %d, %d) ", address, pfn, get_access_cnt(page), nr_migrated_pollute_pages);
					if (address == (end_addr - PAGE_SIZE)) {
						printk("\n");
					}
#endif
				}
			} else {
				set_prcp_flags(page, FLAG_NOT_FIT_IN_CACHE_CHECK);
			}
		}	
	}
	return nr_migrated_pollute_pages;
}

static inline unsigned int prcp_page_migration(struct task_struct *tsk, int core)
{
	unsigned long target_address;
	struct mm_struct *mm = get_task_mm(tsk);
	struct vm_area_struct *vma = mm->mmap;
    pte_t *ptep;
	pte_t pte;
	spinlock_t *ptl;
	int normal_alloc_order = 0;
	int pollute_alloc_order = 0;
	int nr_migrated_normal_pages = 0;
	int nr_migrated_pollute_pages = 0;
	int nr_migrated_cold_pages = 0;
#if DEBUG > 0
	int nr_normal_low_pages = 0;
	int nr_pollute_high_pages = 0;
	int nr_skipped_pages = 0;
#endif
	//struct list_head normal_list[NR_NORMAL_COLORS];
	//struct list_head pollute_list[NR_POLLUTE_COLORS];
	struct list_head migration_list;
	struct page *page;
	unsigned long pfn = 0;
	unsigned int color;
	int color_idx;
	int access_cnt;
	int start_access_cnt = 0;
#if WORKING_SET_TOO_LARGE
	int working_set_size = 0;
	unsigned long start_addr = 0;
	unsigned long end_addr = 0;
#endif
	INIT_LIST_HEAD(&migration_list);
	//INIT_LIST_HEADS(normal_list, NR_NORMAL_COLORS);
	//INIT_LIST_HEADS(pollute_list, NR_POLLUTE_COLORS);
	for (; vma; vma = vma->vm_next) {
		if (check_vma_skippable(vma)) {
			continue;
		}
		for (target_address = vma->vm_start; target_address < vma->vm_end; target_address+=PAGE_SIZE) {
			ptep = get_pte_from_vpa_lock(mm, target_address, &ptl);
			if (ptep == NULL) {
#if WORKING_SET_TOO_LARGE
				nr_migrated_pollute_pages += prcp_detect_working_set_too_large(mm, tsk, vma, working_set_size, start_addr, end_addr, &migration_list);
				init_working_set_too_large_detector(&start_addr, &end_addr, &working_set_size, &start_access_cnt);
#endif
				break;
			}
			pte = *ptep;
			if (pte_present(pte)){
				int is_pollute;
				pfn = pte_pfn(pte);
				page = pfn_to_page(pfn);
				if (get_prcp_flags(page) == FLAG_NOT_MOVABLE || get_prcp_flags(page) == FLAG_NOT_FIT_IN_CACHE) {
#if WORKING_SET_TOO_LARGE
					nr_migrated_pollute_pages += prcp_detect_working_set_too_large(mm, tsk, vma, working_set_size, start_addr, end_addr, &migration_list);
					init_working_set_too_large_detector(&start_addr, &end_addr, &working_set_size, &start_access_cnt);
#endif
					pte_unmap_unlock(ptep, ptl);
#if DEBUG > 0
					nr_skipped_pages++;
#endif
					continue;
				}
				color = prcp_get_color(pfn);
				access_cnt = get_access_cnt(page);
				is_pollute = is_pollute_color(color);

				if ((!is_pollute) && (access_cnt > TOO_LARGE_THRESHOLD)) { //&& ((start_access_cnt == 0) || (get_difference_between_access_cnts(access_cnt, start_access_cnt) < 3))) { // Normal + High
#if WORKING_SET_TOO_LARGE
					if (!start_addr) {
						start_addr = target_address;
						//start_access_cnt = access_cnt;
					}
					end_addr = target_address + PAGE_SIZE;
					working_set_size++;
#endif
				} else {
#if WORKING_SET_TOO_LARGE
					nr_migrated_pollute_pages += prcp_detect_working_set_too_large(mm, tsk, vma, working_set_size, start_addr, end_addr, &migration_list);
					init_working_set_too_large_detector(&start_addr, &end_addr, &working_set_size, &start_access_cnt);
#endif
					if ((!is_pollute) && (access_cnt < LOW_THRESHOLD) && (access_cnt > 0)) { // Normal + Low
						if (get_prcp_flags(page) == FLAG_POLLUTE) {
							if (nr_migrated_cold_pages < NR_MIGRATE_PAGES_AT_ONCE) {
								if (prcp_try_isolate_lru_page(vma, pfn)) {
									init_prcp_flags(page);
									nr_migrated_cold_pages++;
									color_idx = NR_NORMAL_COLORS + (pollute_alloc_order % NR_POLLUTE_COLORS);
									pollute_alloc_order++;
									page->demanded_color = color_idx;
									list_add_tail(&page->lru, &migration_list);
									//list_add_tail(&page->lru, &pollute_list[color_idx]);
								}
							}
						} else {
							set_prcp_flags(page, FLAG_POLLUTE);
						}
#if DEBUG > 0
						nr_normal_low_pages++;
#endif
					} else if (is_pollute && (access_cnt > HIGH_THRESHOLD)) { // Pollute + High
						if (get_prcp_flags(page) == FLAG_NORMAL) {
							if (nr_migrated_normal_pages < NR_MIGRATE_PAGES_AT_ONCE) {
								if (prcp_try_isolate_lru_page(vma, pfn)) {
									init_prcp_flags(page);
									nr_migrated_normal_pages++;
									color_idx = normal_alloc_order % NR_NORMAL_COLORS;
									normal_alloc_order++;
									page->demanded_color = color_idx;
									list_add_tail(&page->lru, &migration_list);
									//list_add_tail(&page->lru, &normal_list[color_idx]);
								}
							}
						} else {
							set_prcp_flags(page, FLAG_NORMAL);
						}
#if DEBUG > 0
						nr_pollute_high_pages++;
#endif
					} 
#if 0
					else {
						if (get_prcp_flags(page) != FLAG_DEFAULT) {
							set_prcp_flags(page, FLAG_DEFAULT);
						}
					}
#endif
				}
			}
			pte_unmap_unlock(ptep, ptl);
		}
#if WORKING_SET_TOO_LARGE
		nr_migrated_pollute_pages += prcp_detect_working_set_too_large(mm, tsk, vma, working_set_size, start_addr, end_addr, &migration_list);
		init_working_set_too_large_detector(&start_addr, &end_addr, &working_set_size, &start_access_cnt);
#endif
	}
	if (!list_empty(&migration_list)) {
		prcp_migrate_pages(tsk, &migration_list, prcp_alloc_migrate_target, 0, MIGRATE_SYNC, MR_NUMA_MISPLACED);
		flush_tlb_mm(mm);
	}
#if 0
	for (i = 0; i < NR_NORMAL_COLORS; i++) {
		if (!list_empty(&normal_list[i])) {
			prcp_migrate_pages(tsk, &normal_list[i], prcp_alloc_migrate_target, (unsigned long)i, MIGRATE_SYNC, MR_NUMA_MISPLACED);
			flush_tlb_mm(mm);
		}
	}

	for (i = 0; i < NR_POLLUTE_COLORS; i++) {
		if (!list_empty(&pollute_list[i])) {
			prcp_migrate_pages(tsk, PRCP_NO_LIMIT, &pollute_list[i], prcp_alloc_migrate_target, (unsigned long)(NR_NORMAL_COLORS + i), MIGRATE_SYNC, MR_NUMA_MISPLACED);
			flush_tlb_mm(mm);
		}
	}
#endif
	local_nr_migrated_pages[core] = nr_migrated_cold_pages;
#if DEBUG > 0
	printk("[prcp_page_migration] For [%s] nr_normal_too_large_pages: %d, nr_normal_low_pages: %d, nr_pollute_high_pages: %d,  nr_skipped_pages: %d, nr_migrated_normal_pages: %d, nr_migrated_pollute_pages: %d, nr_migrated_cold_pages: %d\n", tsk->comm, nr_migrated_pollute_pages, nr_normal_low_pages, nr_pollute_high_pages, nr_skipped_pages, nr_migrated_normal_pages, nr_migrated_pollute_pages, nr_migrated_cold_pages);
#endif
	mmput(mm);
	return 0;
}
static inline unsigned long get_jumping_variance(unsigned long jumping_weight)
{
	unsigned long var = 0;
	unsigned long my_weight = (jumping_weight > 32)? 32 : jumping_weight;
	get_random_bytes(&var, sizeof(unsigned long));
	var = (var % ((my_weight/4)+1));
	return var;
}

static inline int pte_accessed(pte_t pte)
{
	if (pte_present(pte))
		if (pte_young(pte))
			return 1;
	return 0;
}

static inline int prcp_prepare_scan(struct mm_struct *mm)
{
	unsigned long target_address;
	struct vm_area_struct *vma = mm->mmap; 
    pte_t *ptep;
	pte_t pte;
	spinlock_t *ptl;
	for (; vma; vma = vma->vm_next) {
		if (check_vma_skippable(vma))
			continue;
		for (target_address = vma->vm_start; target_address < vma->vm_end; target_address+=PAGE_SIZE) {
			ptep = get_pte_from_vpa_lock(mm, target_address, &ptl);
			if (ptep == NULL) {
				break;
			}
			pte = *ptep;
			if (pte_present(pte)){
				if (pte_young(pte)) {
					pte = pte_mkold(pte);
					set_pte(ptep, pte);
				}
			}
			pte_unmap_unlock(ptep, ptl);
		}
	}
    return 0;
}

int prcp_page_table_scan(void *args)
{
	struct prcp_worker_args *data = args;
	unsigned long target_address;
	struct task_struct *tsk;
	struct mm_struct *mm;
	struct vm_area_struct *vma;
	struct page *page;
	pte_t *ptep;
	pte_t pte;
	spinlock_t *ptl;
	int my_core = smp_processor_id();
	int cur_access_cnt;
	int my_highest_access_cnt;
	int my_sum_access_cnt;
	int my_nr_pages;
	int my_nr_normal_pages;
	int my_nr_pollute_pages;

#if DEBUG > 0
	int average;
#endif
#ifdef DEBUG_TIME
	signed long long start_time, end_time;
#endif

	while(!kthread_should_stop()) {
		wait_event_interruptible(data->private->worker_wq, (data->private->worker_state != WORKER_WAIT)); // wait here until all other threads are ready
		if (data->private->worker_state == WORKER_EXIT)
			goto worker_exit;
		else
			data->private->worker_state = WORKER_WAIT;

		tsk = data->target_tsk;
		if (tsk)
			get_task_struct(tsk);
		else
			goto no_mm_struct;

		mm = get_task_mm(tsk);
		if (!mm) {// if kernel thread
			goto no_mm_struct;
		}

#ifdef DEBUG_TIME
		start_time = get_time();
#endif

		if (unlikely(prcp_stage == STAGE_PAGE_MIGRATION))
			prcp_page_migration(tsk, my_core);

		prcp_prepare_scan(mm);

#ifdef DEBUG_TIME
		end_time = get_time();
		printk("[%s] prcp_prepare_scan()'s running time: %lld\n", tsk->comm, end_time - start_time);
#endif
		usleep_range(SCAN_TIME_WINDOW_US, SCAN_TIME_WINDOW_US);

#ifdef DEBUG_TIME
		start_time = get_time();
#endif
		vma = mm->mmap;

		if (unlikely(prcp_stage == STAGE_PREPARE_DECISION)) {
			unsigned int color;
			unsigned long pfn;
			my_highest_access_cnt = 0;
			my_sum_access_cnt = 0;
			my_nr_pages = 0;
			my_nr_normal_pages = 0;
			my_nr_pollute_pages = 0;
			for (; vma; vma = vma->vm_next) {
				if (check_vma_skippable(vma))
					continue;
				for (target_address = vma->vm_start; target_address < vma->vm_end; target_address+=PAGE_SIZE) {
					ptep = get_pte_from_vpa_lock(mm, target_address, &ptl);
					if (ptep == NULL) {
						break;
					}
					pte = *ptep;
					if (pte_present(pte)) {
						pfn = pte_pfn(pte);
						page = pfn_to_page(pfn);
						if (pte_young(pte)) {
							increase_access_cnt(page);
						}
						cur_access_cnt = get_access_cnt(page);
						color = prcp_get_color(pfn);
						if (is_pollute_color(color)) {
							my_nr_pollute_pages++;
						} else {
							my_nr_normal_pages++;
						}
						if (cur_access_cnt) {
							if (my_highest_access_cnt < cur_access_cnt) {
								my_highest_access_cnt = cur_access_cnt;
							}
							my_sum_access_cnt += cur_access_cnt;
							my_nr_pages++;
						}
					}
					pte_unmap_unlock(ptep, ptl);
				}
			}
			local_highest_access_cnt[my_core] = my_highest_access_cnt;
			local_sum_access_cnt[my_core] = my_sum_access_cnt;
			local_nr_pages[my_core] = my_nr_pages;
			local_nr_pollute_pages[my_core] = my_nr_pollute_pages;
			local_nr_normal_pages[my_core] = my_nr_normal_pages;
#if DEBUG > 0
			average = (my_nr_pages > 0)? my_sum_access_cnt / my_nr_pages : -1;
			printk("[prcp_prepare_decision] For [%s], local_highest_access_cnt: %d, local_average_access_cnt: %d, local_nr_total_pages: %d, local_nr_pollute_pages: %d, local_nr_normal_pages: %d\n", tsk->comm, my_highest_access_cnt, average, my_nr_pollute_pages+my_nr_normal_pages, my_nr_pollute_pages, my_nr_normal_pages);
#endif
		} else if (unlikely(prcp_stage == STAGE_PAGE_MIGRATION)) {
			for (; vma; vma = vma->vm_next) {
				if (check_vma_skippable(vma))
					continue;
				for (target_address = vma->vm_start; target_address < vma->vm_end; target_address+=PAGE_SIZE) {
					ptep = get_pte_from_vpa_lock(mm, target_address, &ptl);
					if (ptep == NULL) {
						break;
					}
					pte = *ptep;
					if (pte_present(pte)) {
						page = pte_page(pte);
						if (get_access_cnt(page)) {
							init_access_cnt(page);
						}
					}
					pte_unmap_unlock(ptep, ptl);
				}
			}
		}
#if LOCALITY_JUMPING
		else {
			for (; vma; vma = vma->vm_next) {
				unsigned long jumping_weight = 1;
				unsigned long jumping_var = 0;
				unsigned long last_non_accessed_addr = -1;
				if (check_vma_skippable(vma))
					continue;
				for (target_address = vma->vm_start; target_address < vma->vm_end; target_address+=((jumping_weight-jumping_var)*PAGE_SIZE)) {
					ptep = get_pte_from_vpa_lock(mm, target_address, &ptl);
					if (ptep == NULL) {
						break;
					}
					pte = *ptep;
					if (pte_accessed(pte)) { // accessed
						if (jumping_weight != 1) {
							jumping_weight = 1;
							if (last_non_accessed_addr != -1) {
								target_address = last_non_accessed_addr;
								last_non_accessed_addr = -1;
							}
						}
						page = pte_page(pte);
						increase_access_cnt(page);
					} else { // not accessed
						if (jumping_weight < 64) {
							jumping_weight = jumping_weight << 1;
							last_non_accessed_addr = target_address;
						}
					}
					jumping_var = get_jumping_variance(jumping_weight);
					pte_unmap_unlock(ptep, ptl);
				}
			}
		}
#else
		else {
			for (; vma; vma = vma->vm_next) {
				if (check_vma_skippable(vma))
					continue;
				for (target_address = vma->vm_start; target_address < vma->vm_end; target_address+=PAGE_SIZE) {
					ptep = get_pte_from_vpa_lock(mm, target_address, &ptl);
					if (ptep == NULL) {
						break;
					}
					pte = *ptep;
					if (pte_accessed(pte)) { // accessed
						page = pte_page(pte);
						increase_access_cnt(page);
					}
					pte_unmap_unlock(ptep, ptl);
				}
			}
		}
#endif

#ifdef DEBUG_TIME
		end_time = get_time();
		printk("[%s] prcp_page_table_scan()'s running time: %lld\n", tsk->comm, end_time - start_time);
#endif
		mmput(mm);
		put_task_struct(tsk);

no_mm_struct:
		atomic_dec(&data->global->thread_count);
worker_exit:
		wake_up_interruptible(&data->global->manager_wq);
	}

	data->private->worker_state = WORKER_TERMINATED;
	printk("[prcp_worker_thread] the worker thread on <core %d> terminated gracefully\n", my_core);
	wake_up_interruptible(&data->global->manager_wq);
	return 0;
}

int prcp_manager_thread(void *args) 
{
	unsigned int i;
	unsigned int cur_turn = 0;
	struct prcp_global_data global;
	struct prcp_private_data private[NR_CORES_PER_NODE];
	struct prcp_worker_args data[NR_CORES_PER_NODE];
	struct task_struct *worker[NR_CORES_PER_NODE];

#ifdef DEBUG_TIME
	signed long long start_time = 0;
	signed long long end_time = 0;
#endif
	printk("[DCSLAB]prcp_manager_thread start\n");
	prcp_init_worker_args(data, &global, &private[0], NR_CORES_PER_NODE);
	
	//create worker threads for each core
	for (i = 0; i < NR_CORES_PER_NODE; i++) {
		worker[i] = kthread_create(prcp_page_table_scan, &data[i], "prcp_worker[%d]", i);
		if (!IS_ERR(worker[i])) {
			kthread_bind(worker[i], i);
			wake_up_process(worker[i]);
		} else {
			printk("[prcp_manager_thread] creating worker thread[%d] failed\n", i);
		}
	}

	while(!kthread_should_stop()) {
		atomic_set(&global.thread_count, NR_CORES_PER_NODE);

		if (unlikely(cur_turn == JUST_BEFORE_DECISION)) {
			prcp_stage = STAGE_PREPARE_DECISION;
		} else if (unlikely(prcp_stage == STAGE_PREPARE_DECISION)) {
			prcp_stage = STAGE_PAGE_MIGRATION;
		} else if (unlikely(prcp_stage == STAGE_PAGE_MIGRATION)) {
			prcp_stage = STAGE_DEFAULT;
			cur_turn = 0;
		}

		for (i = 0; i < NR_CORES_PER_NODE; i++) {
			if (data[i].private->worker_state == WORKER_WAIT) {
				data[i].target_tsk = get_rq_task(i);
				data[i].private->worker_state = WORKER_START;
				wake_up_interruptible(&data[i].private->worker_wq);
			}
		}
		if (wait_event_interruptible_timeout(global.manager_wq, (atomic_read(&global.thread_count) == 0), MANAGER_TIMEOUT * HZ) == 0) {
			printk("[prcp_manager_thread] some of workers do not answer\n");
			goto preparation_for_exit;
		}

		if (unlikely(prcp_stage == STAGE_PREPARE_DECISION)) {
			update_threshold();
			adjust_pollute_buffer();
		}
		cur_turn++;
		if (unlikely(prcp_stage == STAGE_PAGE_MIGRATION)) {
			if (get_total_nr_migrated_pages() < MIGRATION_THRESHOLD) {
				DELAY_WEIGHT++;
			} else {
				DELAY_WEIGHT = 1;
			}
#if DEBUG > 0
			printk("[prcp_manager_thread] DELAY_WEIGHT = %u\n", DELAY_WEIGHT);
#endif
			msleep(MIGRATION_DELAY_UNIT * DELAY_WEIGHT);
		} else {
			msleep(SCAN_FREQUENCY_MS);
		}
	}

preparation_for_exit:
	for (i = 0; i < NR_CORES_PER_NODE; i++) {
		if (worker[i]) {
			data[i].private->worker_state = WORKER_EXIT;
			kthread_stop(worker[i]);
			wake_up_interruptible(&data[i].private->worker_wq);
		}
	}
	printk("[prcp_manager_thread] waiting for all worker threads to stop gracefully\n");
	if (wait_event_interruptible_timeout(global.manager_wq, (prcp_check_all_worker_terminated(&private[0]) == 1), MANAGER_TIMEOUT * HZ) == 0) {

		printk("[prcp_manager_thread] failed to gracefully terminate all workers\n");
	}

	printk("[DCSLAB]prcp_manager_thread end\n");
	return 0;
}

static int scan_init(void)
{
#ifdef DEBUG_FILE
	if (output == NULL) {
		sprintf(buf, "result.csv");
	} else {
		sprintf(buf, output);
	}
	if (!(fp = file_open(buf, O_WRONLY|O_CREAT, 0644))) {
		printk("prcp_scan_init: fd open failed\n");
		return 0;
	} else {
		int i;
		sprintf(buf, "Name,Turn,");
		for (i = 0; i < NR_COLORS_IN_CACHE; i++) {
			sprintf(buf + strlen(buf), "%d,",i);
		}
		sprintf(buf + strlen(buf), "\n");
		file_write(fp, buf, strlen(buf), &offset);
	}
#endif
#if USE_SPLIT_PTE_PTLOCKS
	printk(KERN_ALERT "[DCSLAB]USE_SPLIT_PTE_PTLOCKS\n");
#endif
#if ALLOC_SPLIT_PTLOCKS
	printk(KERN_ALERT "[DCSLAB]ALLOC_SPLIT_PTE_PTLOCKS\n");
#endif
	printk(KERN_ALERT "[DCSLAB]prcp start\n");
	//NR_POLLUTE_COLORS = (NR_COLORS_IN_CACHE >> 4);
	NR_POLLUTE_COLORS = 1;
	NR_NORMAL_COLORS = NR_COLORS_IN_CACHE - NR_POLLUTE_COLORS; // == POLLUTE_COLOR
	prcp_manager = kthread_create(prcp_manager_thread, NULL, "prcp_manager");
	if (!IS_ERR(prcp_manager)) {
		//kthread_bind(prcp_manager, 1);
		kthread_bind(prcp_manager, 3);
		wake_up_process(prcp_manager);
	}
#ifdef CONFIG_FLATMEM
	printk("CONFIG_FLATMEM\n");
#endif
	return 0;
}

static void scan_exit(void)
{
#ifdef DEBUG_FILE
	if (fp) {
		file_close(fp);
	}
#endif
	if (prcp_manager)
		kthread_stop(prcp_manager);
	printk(KERN_ALERT "[DCSLAB]prcp stop\n");
}

module_init(scan_init);
module_exit(scan_exit);
MODULE_LICENSE("GPL");
