#include <linux/migrate.h>
#include <linux/memory_hotplug.h>
#include <linux/pagemap.h>
#include <linux/rmap.h>
#include <linux/init.h>
#include <linux/cpu.h>
#include <linux/percpu.h>
#include <linux/module.h>
#include <linux/kthread.h>
#include <linux/wait.h>
#include <linux/kernel.h>
#include <linux/ioport.h>
#include <asm/io.h>
#include <linux/delay.h>
#include <linux/sched.h>
#include <linux/moduleparam.h>
#include <linux/mm.h>
#include <asm/pgtable.h>
#include <linux/spinlock.h>
#include <linux/slab.h>
#include <linux/mm.h>
#include <asm-generic/pgtable.h>

//[DCSLAB]
#include <linux/lockdep.h>
#include <asm/tlbflush.h>
#include <linux/time.h>
#include <linux/mmu_notifier.h>
#include <linux/preempt.h>
#include "../kernel/sched/sched.h"
#include "../mm/internal.h"
#include <linux/mm_inline.h>
#include <linux/migrate.h>
#include <linux/migrate_mode.h>
#include <linux/hugetlb.h>


//#define DEBUG_FILE 1

#ifdef DEBUG_FILE
#include <linux/file.h>
#include <linux/fs.h>
#include <linux/syscalls.h>
#include <linux/fcntl.h>
#include <asm/uaccess.h>
#endif

//#define DEBUG_TIME

#define DEBUG 1
#define NR_CORES_PER_NODE 2
#define NR_NODES_IN_SYSTEM 1
#define NR_COLORS_IN_CACHE NUM_OF_COLORS
//#define NR_POLLUTE_COLORS 4
//#define NR_POLLUTE_COLORS (NR_COLORS_IN_CACHE >> 4)
#define NR_POLLUTE_COLORS 4
#define NR_NORMAL_COLORS (NR_COLORS_IN_CACHE - NR_POLLUTE_COLORS)
#define POLLUTE_COLOR (NR_COLORS_IN_CACHE - NR_POLLUTE_COLORS)
//#define NR_NORMAL_COLORS NR_COLORS_IN_CACHE
#define DEBUG_DURATION 128
#define SCAN_TIME_WINDOW_US 1000
#define SCAN_FREQUENCY_MS 200

#define NR_ACCESS_HISTORY_LOCAL 32
#define DECISION_TURN (NR_ACCESS_HISTORY_LOCAL - 1)
#define HIGH_THRESHOLD (NR_ACCESS_HISTORY_LOCAL >> 2)
#define LOW_THRESHOLD (NR_ACCESS_HISTORY_LOCAL >> 4)

#define NR_VICTIM_COLORS (NR_COLORS_IN_CACHE >> 2)
//#define NR_VICTIM_COLORS 7
#define NR_TARGET_COLORS (NR_COLORS_IN_CACHE >> 2)
//#define NR_TARGET_COLORS 7

#define NR_MIGRATE_PAGES_AT_ONCE (256)
#define NO_COLOR (-1)

#define FLAG_NORMAL 1
#define FLAG_HOT 2
#define FLAG_POLLUTE 4
#define FLAG_NOT_MOVABLE 7

extern struct task_struct *get_rq_task(int cpu);
extern struct page *cadpm_alloc_migrate_target(struct page *page, unsigned long demanded_color, int **resultp);
extern int cadpm_migrate_page(struct page *old_page, new_page_t get_new_page, unsigned long private, enum migrate_mode mode, int reason);
struct task_struct *cadpm_manager;

static char *output;
unsigned int make_decision = 0;


#ifdef DEBUG_FILE
struct file* fp;
loff_t offset = 0;
char buf[1024];
#endif

module_param(output, charp, 0);

#ifdef DEBUG_FILE
static struct file* file_open(const char *path, int flags, int rights)
{
	struct file* filp = NULL;
	mm_segment_t oldfs;
	int err = 0;

	oldfs = get_fs();
	set_fs(get_ds());
	filp = filp_open(path, flags, rights);
	set_fs(oldfs);
	if (IS_ERR(filp)) {
		err = PTR_ERR(filp);
		return NULL;
	}
	return filp;
}

static void file_close(struct file* file)
{
	filp_close(file, NULL);
}

static int file_write(struct file* file, unsigned char* data, unsigned int size, loff_t* off)
{
	mm_segment_t oldfs;
	int ret;
	oldfs = get_fs();
	set_fs(get_ds());

	ret = vfs_write(file, data, size, off);

	set_fs(oldfs);
	return ret;
}
#endif

struct cadpm_shared_data {
	atomic_t thread_count;
	wait_queue_head_t manager_wq;
	unsigned int cur_turn;
};

struct cadpm_worker_args {
	struct cadpm_shared_data *sd;
	wait_queue_head_t *worker_wq;
	struct task_struct *target_tsk;
	unsigned int *start_flag;
};

static inline void cadpm_init_shared_data(struct cadpm_shared_data *sd)
{
	atomic_set(&sd->thread_count, 0);
	init_waitqueue_head(&sd->manager_wq);
	sd->cur_turn = 0;
}

static inline void cadpm_init_worker_args(struct cadpm_worker_args* data, struct cadpm_shared_data *sd)
{
	int i;
	cadpm_init_shared_data(sd);
	for (i=0; i<NR_CORES_PER_NODE; i++) {
		data[i].sd = sd;
		data[i].worker_wq = NULL;
		data[i].target_tsk = NULL;
		data[i].start_flag = NULL;
	}
}

static inline signed long long get_time(void)
{
	struct timespec ts;
	getnstimeofday(&ts);
	return timespec_to_ns(&ts);
}

static inline unsigned int cadpm_get_color(unsigned long pfn)
{
	return pfn & (NR_COLORS_IN_CACHE - 1);
}

static inline pte_t* get_pte_from_vpa_lock(struct mm_struct *target_mm, unsigned long target_address, spinlock_t **ptl)
{
	struct mm_struct *mm = target_mm;
	unsigned long address = target_address;
    pgd_t *pgd;
    pud_t *pud;
    pmd_t *pmd;

	pgd = pgd_offset(mm, address);
	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
		return NULL;
	pud = pud_offset(pgd, address);
	if (pud_none(*pud) || unlikely(pud_bad(*pud)))
		return NULL;
	pmd = pmd_offset(pud, address);
	if (pmd_none(*pmd) || unlikely(pmd_bad(*pmd)))
		return NULL;
	return pte_offset_map_lock(mm, pmd, address, ptl);
}

static inline int is_pollute_color(unsigned int color)
{
	unsigned int i;
	for (i = POLLUTE_COLOR; i < NR_COLORS_IN_CACHE; i++) {
		if (color == i) {
			return 1;
		}
	}
	return 0;
}

static inline void INIT_LIST_HEADS(struct list_head *list, unsigned int nr_lists)
{
	int i;
	for (i = 0; i < nr_lists; i++) {
		INIT_LIST_HEAD(&list[i]);
	}
}

static inline struct page *cadpm_try_isolate_lru_page(struct vm_area_struct *vma, unsigned long pfn, pte_t *ptep)
{
	struct page *page;
	pte_t pte;
	int ret = 0;
	if (!pfn_valid(pfn))
		return NULL;
	page = pfn_to_page(pfn);

	if (PageHuge(page))
		return NULL;

	if (page_mapcount(page) != 1 && page_is_file_cache(page) &&
	    (vma->vm_flags & VM_EXEC))
		return NULL;

	if (!get_page_unless_zero(page))
		return NULL;
	ret = isolate_lru_page(page);
	if (!ret) { /* Success */
		put_page(page);
		inc_zone_page_state(page, NR_ISOLATED_ANON +
					page_is_file_cache(page));
		return page;
	} else {
		put_page(page);
		pte = pte_set_cadpm_flags(pte, FLAG_NOT_MOVABLE);
		set_pte(ptep, pte);
		return NULL;
	}
}

static inline unsigned int cadpm_page_migration(struct task_struct *tsk, unsigned int cpu)
{
	unsigned long target_address;
	struct mm_struct *mm = get_task_mm(tsk);
	struct vm_area_struct *vma = mm->mmap;
    pte_t *ptep;
	pte_t pte;
	spinlock_t *ptl;
	unsigned int normal_alloc_order = 0;
	//unsigned int hot_alloc_order = 0;
	unsigned int pollute_alloc_order = 0;
	unsigned int nr_migrated_normal_pages = 0;
	//unsigned int nr_migrated_hot_pages = 0;
	unsigned int nr_migrated_pollute_pages = 0;
	struct list_head victim_list[NR_NORMAL_COLORS];
	struct list_head pollute_list[NR_POLLUTE_COLORS];
	struct page *page;
	unsigned int i;
	unsigned long pfn;
	unsigned int color;
	int color_idx;
	int access_cnt;
	INIT_LIST_HEADS(victim_list, NR_NORMAL_COLORS);
	INIT_LIST_HEADS(pollute_list, NR_POLLUTE_COLORS);

	for (; vma; vma = vma->vm_next) {
		if ((vma->vm_flags & (VM_WRITE|VM_EXEC)) == (VM_EXEC)) {// skip text_area
#if DEBUG > 1
			printk("cadpm_page_migration: %s skip text area\n", tsk->comm);
#endif
			continue;
		}
		if (vma->vm_flags & VM_SHARED) {
#if DEBUG > 1
			printk("cadpm_page_migration: %s skip shared area\n", tsk->comm);
#endif
			continue;
		}
		if (vma->vm_file && (vma->vm_flags & (VM_READ|VM_WRITE)) == (VM_READ)) {// skip read-only file-backed mappings
#if DEBUG > 1
			printk("cadpm_page_migration: %s skip read-only file-backed mappings\n", tsk->comm);
#endif
			continue;
		}
#if DEBUG > 1
		printk("cadpm_page_migration: %s vma selected for migration\n", tsk->comm);
#endif
		for (target_address = vma->vm_start; target_address < vma->vm_end; target_address+=PAGE_SIZE) {
			ptep = get_pte_from_vpa_lock(mm, target_address, &ptl);
			if (ptep == NULL) {
				break;
			}
			pte = *ptep;
			if (pte_present(pte)){
				if (pte_get_cadpm_flags(pte) == FLAG_NOT_MOVABLE) {
					pte_unmap_unlock(ptep, ptl);
					continue;
				}
				pfn = pte_pfn(pte);
				color = cadpm_get_color(pfn);
				access_cnt = pte_get_access_cnt(pte);
				if (is_pollute_color(color)) {
					if (access_cnt > HIGH_THRESHOLD) {
						if (pte_get_cadpm_flags(pte) == FLAG_NORMAL) {
							if (nr_migrated_normal_pages < NR_MIGRATE_PAGES_AT_ONCE) {
								if ((page = cadpm_try_isolate_lru_page(vma, pfn, ptep)) != NULL) {
									pte = pte_init_cadpm_flags(pte);
									set_pte(ptep, pte);
									normal_alloc_order++;
									nr_migrated_normal_pages++;
									color_idx = normal_alloc_order % NR_NORMAL_COLORS;
									list_add_tail(&page->lru, &victim_list[color_idx]);
								}
							}
						} else {
							pte = pte_set_cadpm_flags(pte, FLAG_NORMAL);
							set_pte(ptep, pte);
						}
					}
#if 0
					} else {
						if (pte_get_cadpm_flags(pte) == FLAG_NORMAL) {
							pte = pte_init_cadpm_flags(pte);
							set_pte(ptep, pte);
						}
					}
#endif
				} else { // normal colors
					if (access_cnt < LOW_THRESHOLD && access_cnt > 0) {
						if (pte_get_cadpm_flags(pte) == FLAG_POLLUTE) {
							if (nr_migrated_pollute_pages < NR_MIGRATE_PAGES_AT_ONCE) {
								if ((page = cadpm_try_isolate_lru_page(vma, pfn, ptep)) != NULL) {
									pte = pte_init_cadpm_flags(pte);
									set_pte(ptep, pte);
									pollute_alloc_order++;
									nr_migrated_pollute_pages++;
									color_idx = pollute_alloc_order % NR_POLLUTE_COLORS;
									list_add_tail(&page->lru, &pollute_list[color_idx]);
								}
							}
						} else {
							pte = pte_set_cadpm_flags(pte, FLAG_POLLUTE);
							set_pte(ptep, pte);
						}
					}
				}	
			}
			pte_unmap_unlock(ptep, ptl);
		}
	}
	for (i = 0; i < NR_NORMAL_COLORS; i++) {
		if (!list_empty(&victim_list[i])) {
			migrate_pages(&victim_list[i], cadpm_alloc_migrate_target, (unsigned long)i,
					MIGRATE_SYNC, MR_NUMA_MISPLACED);
			flush_tlb_mm(mm);
		}
	}

	for (i = 0; i < NR_POLLUTE_COLORS; i++) {
		if (!list_empty(&pollute_list[i])) {
			migrate_pages(&pollute_list[i], cadpm_alloc_migrate_target, (unsigned long)(POLLUTE_COLOR + i),
					MIGRATE_SYNC, MR_NUMA_MISPLACED);
			flush_tlb_mm(mm);
		}
	}

#if DEBUG > 0
	printk("[cadpm_page_migration] For [%s] nr_migrated_normal_pages: %u, nr_migrated_pollute_pages: %u\n", tsk->comm, nr_migrated_normal_pages, nr_migrated_pollute_pages);
	//printk("[cadpm_page_migration] For [%s] nr_migrated_normal_pages: %u, nr_migrated_pollute_pages: %u, nr_migrated_hot_pages: %u\n", tsk->comm, nr_migrated_normal_pages, nr_migrated_pollute_pages, nr_migrated_hot_pages);
#endif
	return 0;
}

static inline int cadpm_prepare_scan(struct mm_struct *mm)
{
	unsigned long target_address;
	struct vm_area_struct *vma = mm->mmap; 
    pte_t *ptep;
	pte_t pte;
	spinlock_t *ptl;
	for (; vma; vma = vma->vm_next) {
		if ((vma->vm_flags & (VM_WRITE|VM_EXEC)) == (VM_EXEC)) // skip text_area
			continue;
		if (vma->vm_flags & VM_SHARED)
			continue;
		if (vma->vm_file && (vma->vm_flags & (VM_READ|VM_WRITE)) == (VM_READ)) // skip read-only file-backed mappings
			continue;
		for (target_address = vma->vm_start; target_address < vma->vm_end; target_address+=PAGE_SIZE) {
			ptep = get_pte_from_vpa_lock(mm, target_address, &ptl);
			if (ptep == NULL) {
				break;
			}
			pte = *ptep;
			if (pte_present(pte)){
				if (pte_young(pte)) {
					pte = pte_mkold(pte);
					set_pte(ptep, pte);
				}
			}
			pte_unmap_unlock(ptep, ptl);
		}
	}
    return 0;
}

int cadpm_page_table_scan(void *args)
{
	struct cadpm_worker_args *data = args;
	unsigned long target_address;
	unsigned int cur_turn;
	unsigned int need_decay = 0;
	struct task_struct *tsk;
	struct mm_struct *mm;
	struct vm_area_struct *vma;
	pte_t *ptep;
	pte_t pte;
	spinlock_t *ptl;
	unsigned int start_flag;
	wait_queue_head_t wq;

#ifdef DEBUG_TIME
	signed long long start_time, end_time;
#endif

	//init data
	init_waitqueue_head(&wq);
	data->worker_wq = &wq;

	start_flag = 0;
	data->start_flag = &start_flag;

	atomic_dec(&data->sd->thread_count);
	wake_up_interruptible(&data->sd->manager_wq);
	// i'm ready

	while(!kthread_should_stop()) {
		if(wait_event_interruptible_timeout(wq, (start_flag == 1), 5 * HZ) == 0) {// wait here until all other threads are ready
			printk("cadpm_page_table_scan: timeout on core %d\n", smp_processor_id());
			goto preparation_for_exit;
		}
		start_flag = 0;
		cur_turn = data->sd->cur_turn;

		tsk = data->target_tsk;
		mm = get_task_mm(tsk);

		if (!mm) {// if kernel thread
			start_flag = 2;
			goto no_mm_struct;
		}

#ifdef DEBUG_TIME
		if (cur_turn < DEBUG_DURATION) {
			start_time = get_time();
		}
#endif

		cadpm_prepare_scan(mm);

#ifdef DEBUG_TIME
		if (cur_turn < DEBUG_DURATION) {
			end_time = get_time();
			printk("%s[%u]: cadpm_prepare_scan()'s running time: %lld\n", tsk->comm, cur_turn, end_time - start_time);
		}
#endif
		usleep_range(SCAN_TIME_WINDOW_US, SCAN_TIME_WINDOW_US);

#ifdef DEBUG_TIME
		if (cur_turn < DEBUG_DURATION) {
			start_time = get_time();
		}
#endif
		vma = mm->mmap;
		// current_history reset
		for (; vma; vma = vma->vm_next) {
			if ((vma->vm_flags & (VM_WRITE|VM_EXEC)) == (VM_EXEC)) // skip text_area
				continue;
			if (vma->vm_flags & VM_SHARED)
				continue;
			if (vma->vm_file && (vma->vm_flags & (VM_READ|VM_WRITE)) == (VM_READ)) // skip read-only file-backed mappings
				continue;
			for (target_address = vma->vm_start; target_address < vma->vm_end; target_address+=PAGE_SIZE) {
				ptep = get_pte_from_vpa_lock(mm, target_address, &ptl);
				if (ptep == NULL) {
					break;
				}
				pte = *ptep;
				if (pte_present(pte)) {
					if (unlikely(need_decay)) {
#if DEBUG > 1
						if (cur_turn == (128))
							printk("cadpm_decay: Address(%lx)'s pte access count = [before] %d,", target_address, pte_get_access_cnt(*ptep));
#endif
						if (pte_get_access_cnt(pte)) {
							pte = pte_init_access_cnt(pte);
							set_pte(ptep, pte);
						}
#if DEBUG > 1
						if (cur_turn == (128))
							printk("        [after] %d\n", pte_get_access_cnt(*ptep));
#endif
					}
					if (pte_young(pte)){
						pte = pte_increase_access_cnt(pte);
						set_pte(ptep, pte);
					}
				}
				pte_unmap_unlock(ptep, ptl);
			}
		}

		if (unlikely(need_decay))
			need_decay = 0;

		if (unlikely(make_decision)) {
			cadpm_page_migration(tsk, smp_processor_id());
			need_decay = 1;
		}

#ifdef DEBUG_TIME
		if (cur_turn < DEBUG_DURATION) {
			end_time = get_time();
			printk("%s[%u]: cadpm_page_table_scan()'s running time: %lld\n", tsk->comm, cur_turn, end_time - start_time);
		}
#endif
		mmput(mm);

no_mm_struct:
		atomic_dec(&data->sd->thread_count);
		wake_up_interruptible(&data->sd->manager_wq);
	}

preparation_for_exit:
	data->worker_wq = NULL;
	data->start_flag = NULL;
	return 0;
}

int cadpm_manager_thread(void *args) 
{
	unsigned int i;
#if DEBUG > 1
	int k;
#endif
	struct cadpm_shared_data sd;
	struct cadpm_worker_args data[NR_CORES_PER_NODE];
	struct task_struct *worker[NR_CORES_PER_NODE];
#ifdef DEBUG_TIME
	signed long long start_time = 0;
	signed long long end_time = 0;
#endif
	printk("[DCSLAB]cadpm_manager_thread start\n");
	cadpm_init_worker_args(data, &sd);
	
	//create worker threads for each core
	atomic_set(&sd.thread_count, NR_CORES_PER_NODE);
	for (i = 0; i < NR_CORES_PER_NODE; i++) {
		worker[i] = kthread_create(cadpm_page_table_scan, &data[i], "cadpm_worker[%d]", i);
		if (!IS_ERR(worker[i])) {
			kthread_bind(worker[i], i);
			//kthread_bind(worker[i], i+2);
			wake_up_process(worker[i]);
		} else {
			atomic_dec(&sd.thread_count);
		}
	}
	wait_event_interruptible(sd.manager_wq, (atomic_read(&sd.thread_count) == 0));

	// complete creating workers waiting for signal

	while(!kthread_should_stop()) {
		atomic_set(&sd.thread_count, NR_CORES_PER_NODE);

		if (sd.cur_turn == DECISION_TURN) {
			make_decision = 1;
			sd.cur_turn = 0;
		} else {
			make_decision = 0;
		}

		for (i = 0; i < NR_CORES_PER_NODE; i++) {
			if (data[i].start_flag && data[i].worker_wq) {
				data[i].target_tsk = get_rq_task(i);
#if DEBUG > 1
				if (!data[i].target_tsk) {
					printk("cadpm_manager: get_rq_task(%d) failed\n", i);
				} else {
					printk("cadpm_manager: get_rq_task(%d) = %s\n", i, data[i].target_tsk->comm);
				}
#endif
				get_task_struct(data[i].target_tsk);
				if (data[i].start_flag)
					*data[i].start_flag = 1;
				wake_up_interruptible(data[i].worker_wq);
			} else {
				printk("cadpm_manager_thread: one of workers[%u] might have stopped\n", i);
				for (i = i - 1; i >= 0; i--) {
					put_task_struct(data[i].target_tsk); // roll back
				}
				goto preparation_for_exit;
			}
		}
		if (wait_event_interruptible_timeout(sd.manager_wq, (atomic_read(&sd.thread_count) == 0), 5 * HZ) == 0) {
			printk("cadpm_manager_thread: some of workers do not answer\n");
			goto preparation_for_exit;
		}

		for (i = 0; i < NR_CORES_PER_NODE; i++) {
			put_task_struct(data[i].target_tsk);
		}

		sd.cur_turn++;
		msleep(SCAN_FREQUENCY_MS);
	}

preparation_for_exit:
	// need to handle put_task_struct

	for (i = 0; i < NR_CORES_PER_NODE; i++) {
		if (data[i].worker_wq)
			if (worker[i])
				kthread_stop(worker[i]);
	}
	printk("[DCSLAB]cadpm_manager_thread end\n");
	return 0;
}

static int scan_init(void)
{
#ifdef DEBUG_FILE
	if (output == NULL) {
		sprintf(buf, "result.csv");
	} else {
		sprintf(buf, output);
	}
	if (!(fp = file_open(buf, O_WRONLY|O_CREAT, 0644))) {
		printk("cadpm_scan_init: fd open failed\n");
		return 0;
	} else {
		int i;
		sprintf(buf, "Name,Turn,");
		for (i = 0; i < NR_COLORS_IN_CACHE; i++) {
			sprintf(buf + strlen(buf), "%d,",i);
		}
		sprintf(buf + strlen(buf), "\n");
		file_write(fp, buf, strlen(buf), &offset);
	}
#endif
	printk(KERN_ALERT "[DCSLAB]cadpm start\n");
	cadpm_manager = kthread_create(cadpm_manager_thread, NULL, "cadpm_manager");
	if (!IS_ERR(cadpm_manager)) {
		//kthread_bind(cadpm_manager, 1);
		kthread_bind(cadpm_manager, 3);
		wake_up_process(cadpm_manager);
	}
#ifdef CONFIG_FLATMEM
	printk("CONFIG_FLATMEM\n");
#endif
	return 0;
}

static void scan_exit(void)
{
#ifdef DEBUG_FILE
	if (fp) {
		file_close(fp);
	}
#endif
	printk(KERN_ALERT "[DCSLAB]cadpm stop\n");
	if (cadpm_manager)
		kthread_stop(cadpm_manager);
}

module_init(scan_init);
module_exit(scan_exit);
MODULE_LICENSE("GPL");
